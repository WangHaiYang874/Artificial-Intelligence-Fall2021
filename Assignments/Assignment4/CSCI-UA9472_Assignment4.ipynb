{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> CSCI - UA 9472 - Artificial Intelligence </center></h2>\n",
    "\n",
    "<h3><center> Assignment 4: Reinforcement Learning </center></h3>\n",
    "\n",
    "<center>Given date: December 7\n",
    "</center>\n",
    "<center><font color='red'>Due date: December 20/21 </font>\n",
    "</center>\n",
    "<center><b>Total: 20 pts </b>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>In this fourth assignment, we will continue our quest for the optimal agent and replace our heavy logical reasoning model with a faster implementation based on Q-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MonsterKill.png\" width=\"400\" height=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. A simple moving agent (10pts)\n",
    "\n",
    "We consider a simple environment as shown below. Our simple agent has the possibility to move West, East, North and South (except on the borders of the environment). Moreover it has two additional actions:\n",
    "    \n",
    "   - It can hit with its sword (which is essentially useful when facing the skeleton)\n",
    "    \n",
    "   - It can open the door and escape the room (useful when being in the upper rightmost cell)\n",
    "   \n",
    "As you can imagine, hitting while in an empty cell is a loss of energy and should be penalized. We will thus associate useless behaviors (like hitting when alone and opening non existing doors) with a negative reward of -10. Hitting the skeleton and opening the door _when the skeleton has been killed_ should be associated with higher rewards of +30. Escaping without the kill will be associated with a negative reward of -10 (The agent should not exit before getting rid of all the evil in the room)\n",
    "\n",
    "There are a total of 48 cells. Each cell can contain the skeleton, be empty or contain the door (exclusively) which makes a total of $48\\times3 = 144$ states for your environment. Moreover, we want to keep track of whether or not the skeleton has been killed so we will take into account an additional variable encoding whether the kill has been completed. This thus leads to a total of $48\\times 3\\times2 = 288$.\n",
    "\n",
    "Any move will be associated with a small penalty of -1 point    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mazeAssignment3a.png\" width=\"400\" height=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by coding a simple TD Q-learning agent which stores the Q-table entirely as a $288\\times 6$ numpy array. Use an exploration function to avoid being stuck in a limited region of the environment. Recall that such a function can be defined as \n",
    "\n",
    "$$\\begin{align}\n",
    "f(Q_\\theta[s,a], N[s,a]) = \\left\\{\\begin{array}{ll}\n",
    "R^+& \\text{if $N[s,a]<N_e$}\\\\\n",
    "Q_\\theta[s,a]&\\text{otherwise}\n",
    "\\end{array}\\right. \n",
    "\\end{align}$$\n",
    "\n",
    "where $R^+$ is an estimate of the best possible reward that can be obtained in any state.\n",
    "\n",
    "Concretely for each new state and action, you should update the $Q$-table after incrementing the count $N[s,a]$ (of the number of times action $a$ has been taken in state $s$) \n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] & \\leftarrow Q[s,a] +\\eta \\left(N_{sa}[s,a]\\right)\\left(r+\\gamma \\max_{a'} Q[s',a'] - Q[s,a]\\right)\\\\\n",
    "s,a,r &\\leftarrow s', \\underset{a'}{\\operatorname{argmax}} f\\left(Q[s',a'], N[s',a']\\right), r'\n",
    "\\end{align}\n",
    "\n",
    "$\\eta \\left(N_{sa}[s,a]\\right)$ is a learning rate that can be set to a sufficiently small constant or be evolving as the inverse of $N[s, a]$ \n",
    "\n",
    "Consider a couple of episodes to make sure that you fill out a sufficient number of entries in the table. Interlace the learning phases with a couple of evaluation phases during which you should store the number of steps needed to complete the kill and exit so that you will be able to plot the evolution of your agent at the end of the simulation. \n",
    "\n",
    "You can associate a negative reward with the impossible actions corresponding to hitting walls on the borders of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "\n",
    "action_space = ['up','down','left','right','hit','leave']\n",
    "\n",
    "cells = list(itertools.product(np.arange(8),np.arange(6)))\n",
    "cell_states = ['empty','skeleton','door']\n",
    "skeleton_states = ['alive','dead']\n",
    "state_space = list(itertools.product(cells,cell_states,skeleton_states))\n",
    "\n",
    "state2int = {k: v for v, k in enumerate(state_space)}\n",
    "action2int = {k: v for v, k in enumerate(action_space)}\n",
    "\n",
    "door = (7, 5)\n",
    "skeleton = (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .2\n",
    "ne = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions\n",
    "\n",
    "def learning_rate(n):\n",
    "    return 1/(1+n)\n",
    "\n",
    "def f(q, n):\n",
    "    if n < ne: return 10000\n",
    "    return q\n",
    "\n",
    "def action_selection(Q, N, s):\n",
    "    return np.argmax([f(Q[s, a], N[s, a]) for a in range(6)])\n",
    "\n",
    "def next_state(s, a):\n",
    "    '''\n",
    "    all variables are encoded int\n",
    "    this function returns the new state and the reward. \n",
    "    '''\n",
    "    \n",
    "    state = deepcopy(state_space[s])\n",
    "    cell, cell_state, skeleton_state = state\n",
    "    x,y = cell\n",
    "    \n",
    "    action = action_space[a]\n",
    "    reward = -1\n",
    "    # y in range(7), x in range(8)\n",
    "    if action == 'up':        \n",
    "        if y == 5: \n",
    "            reward += -5\n",
    "        else:\n",
    "            y+=1\n",
    "    if action == 'down':\n",
    "        if y == 0: \n",
    "            reward += -5\n",
    "        else:\n",
    "            y-=1\n",
    "    if action == 'left':\n",
    "        if x == 0: \n",
    "            reward += -5\n",
    "        else:\n",
    "            x = x-1\n",
    "    if action == 'right':\n",
    "        if x == 7: \n",
    "            reward += -5\n",
    "        else:\n",
    "            x =  x+1\n",
    "    if action == 'hit':\n",
    "        if cell == skeleton and skeleton_state == 'alive':\n",
    "            reward = 30\n",
    "            skeleton_state = 'dead'\n",
    "        else:\n",
    "            reward = -10\n",
    "    elif action == 'leave':\n",
    "        if cell == door and skeleton_state == 'dead':\n",
    "            return None,30\n",
    "        else:\n",
    "            reward = -10\n",
    "\n",
    "    if (x,y) == door:\n",
    "        cell_state = 'door'\n",
    "    elif (x,y) == skeleton:\n",
    "        cell_state = 'skeleton'\n",
    "    else:\n",
    "        cell_state = 'empty'\n",
    "    \n",
    "    state = ((x,y),cell_state, skeleton_state)\n",
    "    return state2int[state],reward\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evaluations = 100\n",
    "Q = np.zeros((288,6))\n",
    "N = np.zeros((288,6))\n",
    "time_to_completion = np.zeros((num_evaluations,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0: ....................evaluation 16\n",
      "epoch  1: ....................evaluation 18\n",
      "epoch  2: ....................evaluation 14\n",
      "epoch  3: ....................evaluation 20\n",
      "epoch  4: ....................evaluation 18\n",
      "epoch  5: ....................evaluation 14\n",
      "epoch  6: ....................evaluation 16\n",
      "epoch  7: ....................evaluation 14\n",
      "epoch  8: ....................evaluation 14\n",
      "epoch  9: ....................evaluation 36\n",
      "epoch  10: ....................evaluation 14\n",
      "epoch  11: ....................evaluation 14\n",
      "epoch  12: ....................evaluation 14\n",
      "epoch  13: ....................evaluation 14\n",
      "epoch  14: ....................evaluation 14\n",
      "epoch  15: ....................evaluation 14\n",
      "epoch  16: ....................evaluation 14\n",
      "epoch  17: ....................evaluation 42\n",
      "epoch  18: ....................evaluation 16\n",
      "epoch  19: ....................evaluation 14\n",
      "epoch  20: ....................evaluation 14\n",
      "epoch  21: ....................evaluation 14\n",
      "epoch  22: ....................evaluation 14\n",
      "epoch  23: ....................evaluation 14\n",
      "epoch  24: ....................evaluation 14\n",
      "epoch  25: ....................evaluation 14\n",
      "epoch  26: ....................evaluation 14\n",
      "epoch  27: ....................evaluation 46\n",
      "epoch  28: ....................evaluation 14\n",
      "epoch  29: ....................evaluation 14\n",
      "epoch  30: ....................evaluation 14\n",
      "epoch  31: ....................evaluation 14\n",
      "epoch  32: ....................evaluation 14\n",
      "epoch  33: ....................evaluation 14\n",
      "epoch  34: ....................evaluation 14\n",
      "epoch  35: ....................evaluation 14\n",
      "epoch  36: ....................evaluation 14\n",
      "epoch  37: ....................evaluation 14\n",
      "epoch  38: ....................evaluation 36\n",
      "epoch  39: ....................evaluation 16\n",
      "epoch  40: ....................evaluation 16\n",
      "epoch  41: ....................evaluation 14\n",
      "epoch  42: ....................evaluation 16\n",
      "epoch  43: ....................evaluation 14\n",
      "epoch  44: ....................evaluation 14\n",
      "epoch  45: ....................evaluation 14\n",
      "epoch  46: ....................evaluation 14\n",
      "epoch  47: ....................evaluation 14\n",
      "epoch  48: ....................evaluation 14\n",
      "epoch  49: ....................evaluation 14\n",
      "epoch  50: ....................evaluation 14\n",
      "epoch  51: ....................evaluation 16\n",
      "epoch  52: ....................evaluation 14\n",
      "epoch  53: ....................evaluation 14\n",
      "epoch  54: ....................evaluation 14\n",
      "epoch  55: ....................evaluation 14\n",
      "epoch  56: ....................evaluation 14\n",
      "epoch  57: ....................evaluation 14\n",
      "epoch  58: ....................evaluation 18\n",
      "epoch  59: ....................evaluation 14\n",
      "epoch  60: ....................evaluation 14\n",
      "epoch  61: ....................evaluation 14\n",
      "epoch  62: ....................evaluation 14\n",
      "epoch  63: ....................evaluation 14\n",
      "epoch  64: ....................evaluation 14\n",
      "epoch  65: ....................evaluation 14\n",
      "epoch  66: ....................evaluation 14\n",
      "epoch  67: ....................evaluation 14\n",
      "epoch  68: ....................evaluation 14\n",
      "epoch  69: ....................evaluation 14\n",
      "epoch  70: ....................evaluation 14\n",
      "epoch  71: ....................evaluation 14\n",
      "epoch  72: ....................evaluation 14\n",
      "epoch  73: ....................evaluation 14\n",
      "epoch  74: ....................evaluation 14\n",
      "epoch  75: ....................evaluation 14\n",
      "epoch  76: ....................evaluation 14\n",
      "epoch  77: ....................evaluation 14\n",
      "epoch  78: ....................evaluation 14\n",
      "epoch  79: ....................evaluation 14\n",
      "epoch  80: ....................evaluation 14\n",
      "epoch  81: ....................evaluation 14\n",
      "epoch  82: ....................evaluation 14\n",
      "epoch  83: ....................evaluation 14\n",
      "epoch  84: ....................evaluation 14\n",
      "epoch  85: ....................evaluation 16\n",
      "epoch  86: ....................evaluation 14\n",
      "epoch  87: ....................evaluation 14\n",
      "epoch  88: ....................evaluation 14\n",
      "epoch  89: ....................evaluation 14\n",
      "epoch  90: ....................evaluation 14\n",
      "epoch  91: ....................evaluation 14\n",
      "epoch  92: ....................evaluation 14\n",
      "epoch  93: ....................evaluation 14\n",
      "epoch  94: ....................evaluation 14\n",
      "epoch  95: ....................evaluation 14\n",
      "epoch  96: ....................evaluation 14\n",
      "epoch  97: ....................evaluation 14\n",
      "epoch  98: ....................evaluation 14\n",
      "epoch  99: ....................evaluation 14\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_evaluations):\n",
    "    \n",
    "    print('epoch ',k,end=': ')\n",
    "    # train for a couple of epochs\n",
    "    for episode in range(200):\n",
    "        s = 0\n",
    "        while s != None:\n",
    "            a = action_selection(Q,N,s)\n",
    "            s_,r = next_state(s,a)\n",
    "            Q[s,a] = Q[s,a] + learning_rate(N[s,a])*\\\n",
    "                (r + gamma*np.max(Q[s_,:]) -Q[s,a])\n",
    "            N[s,a] += 1\n",
    "            s = s_\n",
    "        if episode%10 == 0:\n",
    "            print('.',end='')\n",
    "        \n",
    "    # Then evaluate the agent \n",
    "\n",
    "    step_count = 0\n",
    "\n",
    "    s = 0    \n",
    "    while s != None:\n",
    "        a = action_selection(Q, N, s)\n",
    "        s_, r = next_state(s, a)\n",
    "        # should I update the Q table here? If I don't update it, it will likely to loop forever. \n",
    "        Q[s, a] = Q[s, a] + learning_rate(N[s, a]) *\\\n",
    "            (r + gamma*np.max(Q[s_, :]) - Q[s, a])\n",
    "        N[s, a] += 1\n",
    "        s = s_\n",
    "        step_count += 1\n",
    "    time_to_completion[k] = step_count\n",
    "    print('evaluation',step_count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'steps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_to_completion)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that my Q-table converged, at the very last epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. The parametric model (15pts)\n",
    "\n",
    "We now want to improve the generalization skills of our agent. We will encode our $Q$-table as a simple feedforward neural network. To avoid losing time with the implementation, we will rely on keras for the architecture (see below). We will consider a **simple one hidden layer** network that takes as **input** a vector encoding the current state of the environment (note that such a vector can be encoded in various ways, for example as a $5$-tuple including the cartesian coordinates of the agent and 3 variables indicating whether there is a door, skeleton,..) and returns as **output** the value associated with each of the actions. In other words we consider a parametric model $f_\\theta\\in \\mathbb{R}^{|a|}$ such that $[f_{\\theta}(s)]_a \\approx Q[s, a]$. Recall that the TD update for such a model is given\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &\\leftarrow \\theta_i + \\alpha \\left[R[s] + \\gamma \\max_{a'} \\hat{Q}_{\\theta}[s',a'] - \\hat{Q}[s,a]\\right]\\frac{\\partial \\hat{Q}_\\theta[s,a]}{\\partial \\theta_i} \n",
    "\\end{align}\n",
    "\n",
    "- Start by updating the weights after each new step (in an SGD fashion) and evaluate the agent as before by interlacing your training with a couple of evaluation step where you should compute the time needed to exit with the kill completed. \n",
    "\n",
    "\n",
    "- We will then be a little wiser and keep track of a short memory before updating the network. \n",
    "\n",
    "Consider the following steps :\n",
    "\n",
    "   - Initialize the memory to an empty vector\n",
    "   \n",
    "    \n",
    "   - As before, we will consider a number of episodes. For each episode, we will consider a full simulation (until exiting with the kill). Again you should rely on some exploration function to alternate between exploration and exploitation (selection of the action corresponding to the maximum output of the network for the current state). \n",
    "   \n",
    "    \n",
    "   - After each action, observe the reward and store the tuple $(s_t, a_t, r_t, s_{t+1})$ and store it in the memory. \n",
    "   \n",
    "    \n",
    "   - Select a random minibatch from the memory and update the networks through a gradient update on the minibatch (mb) \n",
    "    \n",
    "    $$\\min_{\\theta_i} \\frac{1}{\\text{size}(\\text{mb})}\\sum_{t\\in \\text{mb}}\\left(R[s_t] + \\gamma \\max_{a_{t+1}} \\hat{Q}_{\\theta}[s_{t+1},a_{t+1}] - \\hat{Q}_\\theta[s_t,a_t]\\right)^2 $$\n",
    "\n",
    "\n",
    "\n",
    "- To see how to build a Feedforward neural network in keras, the simplest approach is to use the method 'add' such as in\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(n, activation=\"relu\"))\n",
    "\n",
    "    you can see [here](https://keras.io/guides/sequential_model/) for more details.\n",
    "    \n",
    "    \n",
    "    \n",
    "- Once you have built the model, you need to select an optimizer before you can train it. This can be implemented through the ['compile method'](https://keras.io/api/optimizers/)\n",
    "\n",
    "          model.compile(optimizer='SGD',\n",
    "          loss='mean_squared_error',\n",
    "          metrics=['accuracy'])\n",
    "          \n",
    "- Finally to train the network, you can rely on the fit function (see also [here](https://keras.io/api/models/model_training_apis/))\n",
    "\n",
    "        model.fit(train_data, train_labels,epochs=5)\n",
    "        \n",
    "Note that Keras (when using 'compile' with the SGD option) relies on a default batch of size 32 so either you should specify a smaller batch size or you should make sure your memory is sufficiently big. Also note that as written above we will in fact do a coule of SGD iterations (one for each sample in the minibatch, repeated epochs times).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_state(s):\n",
    "    pos, cell_state, skeleton_state = deepcopy(state_space[s])\n",
    "    x,y = pos\n",
    "    ret = [x,y]\n",
    "    ret.append(int(cell_state == 'empty'))\n",
    "    ret.append(int(cell_state == 'skeleton'))\n",
    "    ret.append(int(cell_state == 'door'))\n",
    "    ret.append(int(skeleton_state == 'alive'))\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evaluations = 10\n",
    "time_to_completion = np.zeros((num_evaluations,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    keras.Input(shape=(6,)), # dimension 6 for the state space input. \n",
    "    layers.Dense(32,activation='relu'),# hidden layer\n",
    "    layers.Dense(6) # output the values of each action. \n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD',loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0: ....................evaluation 16\n",
      "epoch  1: ....................evaluation 18\n",
      "epoch  2: ....................evaluation 14\n",
      "epoch  3: ....................evaluation 20\n",
      "epoch  4: ....................evaluation 18\n",
      "epoch  5: ....................evaluation 14\n",
      "epoch  6: ....................evaluation 16\n",
      "epoch  7: ....................evaluation 14\n",
      "epoch  8: ....................evaluation 14\n",
      "epoch  9: ....................evaluation 36\n",
      "epoch  10: ....................evaluation 14\n",
      "epoch  11: ....................evaluation 14\n",
      "epoch  12: ....................evaluation 14\n",
      "epoch  13: ....................evaluation 14\n",
      "epoch  14: ....................evaluation 14\n",
      "epoch  15: ....................evaluation 14\n",
      "epoch  16: ....................evaluation 14\n",
      "epoch  17: ....................evaluation 42\n",
      "epoch  18: ....................evaluation 16\n",
      "epoch  19: ....................evaluation 14\n",
      "epoch  20: ....................evaluation 14\n",
      "epoch  21: ....................evaluation 14\n",
      "epoch  22: ....................evaluation 14\n",
      "epoch  23: ....................evaluation 14\n",
      "epoch  24: ....................evaluation 14\n",
      "epoch  25: ....................evaluation 14\n",
      "epoch  26: ....................evaluation 14\n",
      "epoch  27: ....................evaluation 46\n",
      "epoch  28: ....................evaluation 14\n",
      "epoch  29: ....................evaluation 14\n",
      "epoch  30: ....................evaluation 14\n",
      "epoch  31: ....................evaluation 14\n",
      "epoch  32: ....................evaluation 14\n",
      "epoch  33: ....................evaluation 14\n",
      "epoch  34: ....................evaluation 14\n",
      "epoch  35: ....................evaluation 14\n",
      "epoch  36: ....................evaluation 14\n",
      "epoch  37: ....................evaluation 14\n",
      "epoch  38: ....................evaluation 36\n",
      "epoch  39: ....................evaluation 16\n",
      "epoch  40: ....................evaluation 16\n",
      "epoch  41: ....................evaluation 14\n",
      "epoch  42: ....................evaluation 16\n",
      "epoch  43: ....................evaluation 14\n",
      "epoch  44: ....................evaluation 14\n",
      "epoch  45: ....................evaluation 14\n",
      "epoch  46: ....................evaluation 14\n",
      "epoch  47: ....................evaluation 14\n",
      "epoch  48: ....................evaluation 14\n",
      "epoch  49: ....................evaluation 14\n",
      "epoch  50: ....................evaluation 14\n",
      "epoch  51: ....................evaluation 16\n",
      "epoch  52: ....................evaluation 14\n",
      "epoch  53: ....................evaluation 14\n",
      "epoch  54: ....................evaluation 14\n",
      "epoch  55: ....................evaluation 14\n",
      "epoch  56: ....................evaluation 14\n",
      "epoch  57: ....................evaluation 14\n",
      "epoch  58: ....................evaluation 18\n",
      "epoch  59: ....................evaluation 14\n",
      "epoch  60: ....................evaluation 14\n",
      "epoch  61: ....................evaluation 14\n",
      "epoch  62: ....................evaluation 14\n",
      "epoch  63: ....................evaluation 14\n",
      "epoch  64: ....................evaluation 14\n",
      "epoch  65: ....................evaluation 14\n",
      "epoch  66: ....................evaluation 14\n",
      "epoch  67: ....................evaluation 14\n",
      "epoch  68: ....................evaluation 14\n",
      "epoch  69: ....................evaluation 14\n",
      "epoch  70: ....................evaluation 14\n",
      "epoch  71: ....................evaluation 14\n",
      "epoch  72: ....................evaluation 14\n",
      "epoch  73: ....................evaluation 14\n",
      "epoch  74: ....................evaluation 14\n",
      "epoch  75: ....................evaluation 14\n",
      "epoch  76: ....................evaluation 14\n",
      "epoch  77: ....................evaluation 14\n",
      "epoch  78: ....................evaluation 14\n",
      "epoch  79: ....................evaluation 14\n",
      "epoch  80: ....................evaluation 14\n",
      "epoch  81: ....................evaluation 14\n",
      "epoch  82: ....................evaluation 14\n",
      "epoch  83: ....................evaluation 14\n",
      "epoch  84: ....................evaluation 14\n",
      "epoch  85: ....................evaluation 16\n",
      "epoch  86: ....................evaluation 14\n",
      "epoch  87: ....................evaluation 14\n",
      "epoch  88: ....................evaluation 14\n",
      "epoch  89: ....................evaluation 14\n",
      "epoch  90: ....................evaluation 14\n",
      "epoch  91: ....................evaluation 14\n",
      "epoch  92: ....................evaluation 14\n",
      "epoch  93: ....................evaluation 14\n",
      "epoch  94: ....................evaluation 14\n",
      "epoch  95: ....................evaluation 14\n",
      "epoch  96: ....................evaluation 14\n",
      "epoch  97: ....................evaluation 14\n",
      "epoch  98: ....................evaluation 14\n",
      "epoch  99: ....................evaluation 14\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_evaluations):\n",
    "    \n",
    "    print('epoch ',k,end=': ')\n",
    "    mem = []\n",
    "    for episode in range(200):\n",
    "        s = 0\n",
    "        while s != None:\n",
    "            a = action_selection(Q,N,s)\n",
    "            s_,r = next_state(s,a)\n",
    "            Q[s,a] = Q[s,a] + learning_rate(N[s,a])*\\\n",
    "                (r + gamma*np.max(Q[s_,:]) -Q[s,a])\n",
    "            N[s,a] += 1\n",
    "            s = s_\n",
    "        if episode%10 == 0:\n",
    "            print('.',end='')\n",
    "        \n",
    "    # Then evaluate the agent \n",
    "\n",
    "    step_count = 0\n",
    "\n",
    "    s = 0    \n",
    "    while s != None:\n",
    "        a = action_selection(Q, N, s)\n",
    "        s_, r = next_state(s, a)\n",
    "        # should I update the Q table here? If I don't update it, it will likely to loop forever. \n",
    "        Q[s, a] = Q[s, a] + learning_rate(N[s, a]) *\\\n",
    "            (r + gamma*np.max(Q[s_, :]) - Q[s, a])\n",
    "        N[s, a] += 1\n",
    "        s = s_\n",
    "        step_count += 1\n",
    "    time_to_completion[k] = step_count\n",
    "    print('evaluation',step_count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3. (5pts) \n",
    "\n",
    "Compare and plot the improvement in the behavior of your agent for a couple of neural networks architectures. Don't make the network overly complicated. You can just try different activation functions with different numbers of units per layers (possibly also consider a _small_ number of hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
